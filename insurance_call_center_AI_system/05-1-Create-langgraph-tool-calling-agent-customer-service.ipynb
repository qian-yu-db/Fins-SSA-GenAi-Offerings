{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c17a6a07-18e0-492a-8fe1-f093bc745347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# The Notebook Perform Author and deploy a tool-calling LangGraph agent using Mosaic AI Agent Framework: \n",
    "\n",
    "- Author a UC tool-calling LangGraph agent wrapped with `ChatAgent`\n",
    "- Log and deploy the agent\n",
    "\n",
    "To learn more about authoring an agent using Mosaic AI Agent Framework, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/author-agent) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/create-chat-model))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db253b14-0030-46bf-b7ca-626678ec5ccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqqq mlflow langchain langgraph<0.3.0 databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b96664d-0f7f-473e-a621-f6389c3b71aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Define the agent in code\n",
    "Define the agent code in a single cell below. This lets you easily write the agent code to a local Python file, using the `%%writefile` magic command, for subsequent logging and deployment.\n",
    "\n",
    "#### Agent tools\n",
    "This agent code adds the built-in Unity Catalog function `system.ai.python_exec` to the agent. The agent code also includes commented-out sample code for adding a vector search index to perform unstructured data retrieval.\n",
    "\n",
    "For more examples of tools to add to your agent, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/agent-tool) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/agent-tool))\n",
    "\n",
    "#### Wrap the LangGraph agent using the `ChatAgent` interface\n",
    "\n",
    "For compatibility with Databricks AI features, the `LangGraphChatAgent` class implements the `ChatAgent` interface to wrap the LangGraph agent. This example uses the provided convenience APIs [`ChatAgentState`](https://mlflow.org/docs/latest/python_api/mlflow.langchain.html#mlflow.langchain.chat_agent_langgraph.ChatAgentState) and [`ChatAgentToolNode`](https://mlflow.org/docs/latest/python_api/mlflow.langchain.html#mlflow.langchain.chat_agent_langgraph.ChatAgentToolNode) for ease of use.\n",
    "\n",
    "Databricks recommends using `ChatAgent` as it simplifies authoring multi-turn conversational agents using an open source standard. See MLflow's [ChatAgent documentation](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatAgent).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7c4ff41-0f57-4434-bcd7-e5b362c66d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Find Existing UC Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9473ac1a-482f-4662-aba4-d5466350293f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/databricks/connect/session.py:454: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th></tr></thead><tbody><tr><td>get_customer_intent_by_phone_number</td></tr><tr><td>get_customer_policy_profile_by_phone_number</td></tr><tr><td>get_customer_sentiment_by_phone_number</td></tr><tr><td>get_customer_transcript_by_phone_number</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "get_customer_intent_by_phone_number"
        ],
        [
         "get_customer_policy_profile_by_phone_number"
        ],
        [
         "get_customer_sentiment_by_phone_number"
        ],
        [
         "get_customer_transcript_by_phone_number"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_1",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks_langchain import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "\n",
    "catalog = \"fins_genai\"\n",
    "schema = \"call_center\"\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "uc_tools = client.list_functions(catalog=catalog, schema=schema)\n",
    "uc_tool_names = [tool.name for tool in uc_tools]\n",
    "display(uc_tool_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "669a91c4-0bd8-475b-a262-9cc82738fcf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define LangGraph Tool Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f83ce2c-dfcc-49d6-b73c-eed0b62f3805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting customer_service_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile customer_service_agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import ChatDatabricks, VectorSearchRetrieverTool\n",
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = \"\"\"You are a helpful insurance customer relationship management assistant. Your tasks is to \n",
    "    plan, analyze and give next best action to reponse to the customer based on customers profile, call transcripts, \n",
    "    and the sentiment of the latest call to improve customer happiness and prevent customer churn.\n",
    "\n",
    "    You can use the following tools and use the phone number in the format of '(123)-456-7890'\n",
    "    - Use the get_customer_policy_profile_by_phone_number to get profile of the customer with a give phone number \n",
    "      and show who the customer is and what policy he or she has\n",
    "    - Use the get_customer_intent_by_phone_number to get intent of customer's last call with a given phone number\n",
    "    - Use the get_customer_sentiment_by_phone_number to get sentiment of the customer's last call with a given phone number\n",
    "    - Use the get_customer_transcripts_by_phone_number to get a customer latest call transcript by phone number\n",
    "\n",
    "    Make sure to use the appropriate tool for your accessement, \n",
    "    think step by step, and provide a coherent response to the customer. \n",
    "    Don't mention tools and call transcripts to your users. Only answer what the user is asking for. \n",
    "    If the question isn't related to the tools or insurance, say you're sorry but can't answer\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent, enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://docs.databricks.com/en/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "CATALOG = \"fins_genai\"\n",
    "SCHEMA = \"call_center\"\n",
    "uc_tool_names = [f\"{CATALOG}.{SCHEMA}.{func.name}\" for func in client.list_functions(catalog=CATALOG,\n",
    "                                                                                     schema=SCHEMA)]\n",
    "\n",
    "# You can use UDFs in Unity Catalog as agent tools\n",
    "# Below, we add the `system.ai.python_exec` UDF, which provides\n",
    "# a python code interpreter tool to our agent\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools = uc_toolkit.tools\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolExecutor, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "745dd0f1-a354-4488-8d51-16382a351468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output and tool-calling abilities. Since this notebook called `mlflow.langchain.autolog()`, you can view the trace for each step the agent takes.\n",
    "\n",
    "Replace this placeholder input with an appropriate domain-specific example for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e941046-cce3-4dea-8c14-7ebaefc05df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04671534-0ee9-46f0-9cfc-4ea71c0d01a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/unitycatalog/ai/core/databricks.py:713: UserWarning: The following parameters do not have descriptions: search_name for the function fins_genai.call_center.get_customer_transcript_by_phone_number. Using Unity Catalog functions that do not have parameter descriptions limits the functionality for an LLM to understand how to call your function. To improve tool calling accuracy, provide verbose parameter descriptions that fully explain what the expected usage of the function arguments are.\n  check_function_info(function_info)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/unitycatalog/ai/core/databricks.py:713: UserWarning: The following parameters do not have descriptions: search_name for the function fins_genai.call_center.get_customer_sentiment_by_phone_number. Using Unity Catalog functions that do not have parameter descriptions limits the functionality for an LLM to understand how to call your function. To improve tool calling accuracy, provide verbose parameter descriptions that fully explain what the expected usage of the function arguments are.\n  check_function_info(function_info)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/unitycatalog/ai/core/databricks.py:713: UserWarning: The following parameters do not have descriptions: search_name for the function fins_genai.call_center.get_customer_intent_by_phone_number. Using Unity Catalog functions that do not have parameter descriptions limits the functionality for an LLM to understand how to call your function. To improve tool calling accuracy, provide verbose parameter descriptions that fully explain what the expected usage of the function arguments are.\n  check_function_info(function_info)\n{\"ts\": \"2025-04-26 21:37:17,442\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\n'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\n+- 'SubqueryAlias get_customer_intent_by_phone_number\\n   +- 'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\n      +- 'LateralJoin lateral-subquery#14637 [], Inner\\n         :  +- 'GlobalLimit 1\\n         :     +- 'LocalLimit 1\\n         :        +- 'Sort ['call_timestamp DESC NULLS LAST], true\\n         :           +- 'Project ['intent]\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\n            +- OneRowRelation\\n\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\\\n\\\\'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\\\n+- \\\\'SubqueryAlias get_customer_intent_by_phone_number\\\\n   +- \\\\'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\\\n      +- \\\\'LateralJoin lateral-subquery#14637 [], Inner\\\\n         :  +- \\\\'GlobalLimit 1\\\\n         :     +- \\\\'LocalLimit 1\\\\n         :        +- \\\\'Sort [\\\\'call_timestamp DESC NULLS LAST], true\\\\n         :           +- \\\\'Project [\\\\'intent]\\\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\\\n            +- OneRowRelation\\\\n\\\", grpc_status:13, created_time:\\\"2025-04-26T21:37:17.441960789+00:00\\\"}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py\\\", line 1902, in _execute_and_fetch_as_iterator\", \"    for b in generator:\", \"  File \\\"/usr/lib/python3.10/_collections_abc.py\\\", line 330, in __next__\", \"    return self.send(None)\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 140, in send\", \"    if not self._has_next():\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 201, in _has_next\", \"    raise e\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 173, in _has_next\", \"    self._current = self._call_iter(\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 298, in _call_iter\", \"    raise e\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 278, in _call_iter\", \"    return iter_fun()\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 174, in <lambda>\", \"    lambda: next(self._iterator)  # type: ignore[arg-type]\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py\\\", line 650, in __iter__\", \"    for response in self._call:\", \"  File \\\"/databricks/python/lib/python3.10/site-packages/grpc/_channel.py\\\", line 542, in __next__\", \"    return self._next()\", \"  File \\\"/databricks/python/lib/python3.10/site-packages/grpc/_channel.py\\\", line 968, in _next\", \"    raise self\", \"grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\", \"'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\", \"+- 'SubqueryAlias get_customer_intent_by_phone_number\", \"   +- 'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\", \"      +- 'LateralJoin lateral-subquery#14637 [], Inner\", \"         :  +- 'GlobalLimit 1\", \"         :     +- 'LocalLimit 1\", \"         :        +- 'Sort ['call_timestamp DESC NULLS LAST], true\", \"         :           +- 'Project ['intent]\", \"         :              +- Filter (phone_number#14696 = outer(search_name#14636))\", \"         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\", \"         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\", \"         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\", \"            +- OneRowRelation\", \"\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\\\n\\\\'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\\\n+- \\\\'SubqueryAlias get_customer_intent_by_phone_number\\\\n   +- \\\\'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\\\n      +- \\\\'LateralJoin lateral-subquery#14637 [], Inner\\\\n         :  +- \\\\'GlobalLimit 1\\\\n         :     +- \\\\'LocalLimit 1\\\\n         :        +- \\\\'Sort [\\\\'call_timestamp DESC NULLS LAST], true\\\\n         :           +- \\\\'Project [\\\\'intent]\\\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\\\n            +- OneRowRelation\\\\n\\\", grpc_status:13, created_time:\\\"2025-04-26T21:37:17.441960789+00:00\\\"}\\\"\", \">\"]}}\n{\"ts\": \"2025-04-26 21:37:17,442\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\n'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\n+- 'SubqueryAlias get_customer_intent_by_phone_number\\n   +- 'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\n      +- 'LateralJoin lateral-subquery#14637 [], Inner\\n         :  +- 'GlobalLimit 1\\n         :     +- 'LocalLimit 1\\n         :        +- 'Sort ['call_timestamp DESC NULLS LAST], true\\n         :           +- 'Project ['intent]\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\n            +- OneRowRelation\\n\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\\\n\\\\'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\\\n+- \\\\'SubqueryAlias get_customer_intent_by_phone_number\\\\n   +- \\\\'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\\\n      +- \\\\'LateralJoin lateral-subquery#14637 [], Inner\\\\n         :  +- \\\\'GlobalLimit 1\\\\n         :     +- \\\\'LocalLimit 1\\\\n         :        +- \\\\'Sort [\\\\'call_timestamp DESC NULLS LAST], true\\\\n         :           +- \\\\'Project [\\\\'intent]\\\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\\\n            +- OneRowRelation\\\\n\\\", grpc_status:13, created_time:\\\"2025-04-26T21:37:17.441960789+00:00\\\"}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py\\\", line 1902, in _execute_and_fetch_as_iterator\", \"    for b in generator:\", \"  File \\\"/usr/lib/python3.10/_collections_abc.py\\\", line 330, in __next__\", \"    return self.send(None)\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 140, in send\", \"    if not self._has_next():\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 201, in _has_next\", \"    raise e\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 173, in _has_next\", \"    self._current = self._call_iter(\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 298, in _call_iter\", \"    raise e\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 278, in _call_iter\", \"    return iter_fun()\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 174, in <lambda>\", \"    lambda: next(self._iterator)  # type: ignore[arg-type]\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py\\\", line 650, in __iter__\", \"    for response in self._call:\", \"  File \\\"/databricks/python/lib/python3.10/site-packages/grpc/_channel.py\\\", line 542, in __next__\", \"    return self._next()\", \"  File \\\"/databricks/python/lib/python3.10/site-packages/grpc/_channel.py\\\", line 968, in _next\", \"    raise self\", \"grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\", \"'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\", \"+- 'SubqueryAlias get_customer_intent_by_phone_number\", \"   +- 'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\", \"      +- 'LateralJoin lateral-subquery#14637 [], Inner\", \"         :  +- 'GlobalLimit 1\", \"         :     +- 'LocalLimit 1\", \"         :        +- 'Sort ['call_timestamp DESC NULLS LAST], true\", \"         :           +- 'Project ['intent]\", \"         :              +- Filter (phone_number#14696 = outer(search_name#14636))\", \"         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\", \"         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\", \"         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\", \"            +- OneRowRelation\", \"\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\\\n\\\\'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\\\n+- \\\\'SubqueryAlias get_customer_intent_by_phone_number\\\\n   +- \\\\'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\\\n      +- \\\\'LateralJoin lateral-subquery#14637 [], Inner\\\\n         :  +- \\\\'GlobalLimit 1\\\\n         :     +- \\\\'LocalLimit 1\\\\n         :        +- \\\\'Sort [\\\\'call_timestamp DESC NULLS LAST], true\\\\n         :           +- \\\\'Project [\\\\'intent]\\\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\\\n            +- OneRowRelation\\\\n\\\", grpc_status:13, created_time:\\\"2025-04-26T21:37:17.441960789+00:00\\\"}\\\"\", \">\"]}}\n{\"ts\": \"2025-04-26 21:37:17,442\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\n'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\n+- 'SubqueryAlias get_customer_intent_by_phone_number\\n   +- 'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\n      +- 'LateralJoin lateral-subquery#14637 [], Inner\\n         :  +- 'GlobalLimit 1\\n         :     +- 'LocalLimit 1\\n         :        +- 'Sort ['call_timestamp DESC NULLS LAST], true\\n         :           +- 'Project ['intent]\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\n            +- OneRowRelation\\n\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\\\n\\\\'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\\\n+- \\\\'SubqueryAlias get_customer_intent_by_phone_number\\\\n   +- \\\\'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\\\n      +- \\\\'LateralJoin lateral-subquery#14637 [], Inner\\\\n         :  +- \\\\'GlobalLimit 1\\\\n         :     +- \\\\'LocalLimit 1\\\\n         :        +- \\\\'Sort [\\\\'call_timestamp DESC NULLS LAST], true\\\\n         :           +- \\\\'Project [\\\\'intent]\\\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\\\n            +- OneRowRelation\\\\n\\\", grpc_status:13, created_time:\\\"2025-04-26T21:37:17.441960789+00:00\\\"}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py\\\", line 1902, in _execute_and_fetch_as_iterator\", \"    for b in generator:\", \"  File \\\"/usr/lib/python3.10/_collections_abc.py\\\", line 330, in __next__\", \"    return self.send(None)\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 140, in send\", \"    if not self._has_next():\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 201, in _has_next\", \"    raise e\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\\\", line 173, in _has_next\", \"    self._current = self._call_iter(\", \"  File \\\"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/\n\n*** WARNING: max output size exceeded, skipping output. ***\n\ned#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\n            +- OneRowRelation\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:482)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:176)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:426)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:411)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:411)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:411)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:411)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:414)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkSubqueryExpression(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkSubqueryExpression$(CheckAnalysis.scala:1063)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkSubqueryExpression(Analyzer.scala:414)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:518)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:411)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:411)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:411)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:411)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:414)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:241)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:414)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSQLTableFunctions$$anonfun$apply$32.applyOrElse(Analyzer.scala:3773)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSQLTableFunctions$$anonfun$apply$32.applyOrElse(Analyzer.scala:3752)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1320)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1319)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSQLTableFunctions$.apply(Analyzer.scala:3752)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSQLTableFunctions$.apply(Analyzer.scala:3684)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:484)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:633)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:484)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:483)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:479)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:456)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:589)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:589)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:589)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:506)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:395)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:414)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:345)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:345)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:486)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:486)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:279)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:525)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:623)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:146)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:623)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1257)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:622)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:618)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1444)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:618)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:273)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1755)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1816)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:305)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:253)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:131)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1444)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1451)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1451)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1444)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1076)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3516)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3362)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3297)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:435)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1444)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:435)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:434)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py\", line 1902, in _execute_and_fetch_as_iterator\n    for b in generator:\n  File \"/usr/lib/python3.10/_collections_abc.py\", line 330, in __next__\n    return self.send(None)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\", line 140, in send\n    if not self._has_next():\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\", line 201, in _has_next\n    raise e\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\", line 173, in _has_next\n    self._current = self._call_iter(\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\", line 298, in _call_iter\n    raise e\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\", line 278, in _call_iter\n    return iter_fun()\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/reattach.py\", line 174, in <lambda>\n    lambda: next(self._iterator)  # type: ignore[arg-type]\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py\", line 650, in __iter__\n    for response in self._call:\n  File \"/databricks/python/lib/python3.10/site-packages/grpc/_channel.py\", line 542, in __next__\n    return self._next()\n  File \"/databricks/python/lib/python3.10/site-packages/grpc/_channel.py\", line 968, in _next\n    raise self\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\n'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\n+- 'SubqueryAlias get_customer_intent_by_phone_number\n   +- 'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\n      +- 'LateralJoin lateral-subquery#14637 [], Inner\n         :  +- 'GlobalLimit 1\n         :     +- 'LocalLimit 1\n         :        +- 'Sort ['call_timestamp DESC NULLS LAST], true\n         :           +- 'Project ['intent]\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\n            +- OneRowRelation\n\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\n\\'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\n+- \\'SubqueryAlias get_customer_intent_by_phone_number\\n   +- \\'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\n      +- \\'LateralJoin lateral-subquery#14637 [], Inner\\n         :  +- \\'GlobalLimit 1\\n         :     +- \\'LocalLimit 1\\n         :        +- \\'Sort [\\'call_timestamp DESC NULLS LAST], true\\n         :           +- \\'Project [\\'intent]\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\n            +- OneRowRelation\\n\", grpc_status:13, created_time:\"2025-04-26T21:37:17.441960789+00:00\"}\"\n>\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/unitycatalog/ai/core/databricks.py:713: UserWarning: The following parameters do not have descriptions: search_name for the function fins_genai.call_center.get_customer_policy_profile_by_phone_number. Using Unity Catalog functions that do not have parameter descriptions limits the functionality for an LLM to understand how to call your function. To improve tool calling accuracy, provide verbose parameter descriptions that fully explain what the expected usage of the function arguments are.\n  check_function_info(function_info)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ChatAgentResponse(messages=[ChatAgentMessage(role='assistant', content='', name=None, id='run-8a2954ac-2e81-4b55-afd6-66fe96e8f78c-0', tool_calls=[ToolCall(id='call_6401c21c-4d98-48a1-adf2-4717f9ed7ba1', type='function', function=Function(name='fins_genai__call_center__get_customer_transcript_by_phone_number', arguments='{\"search_name\": \"(875)-209-8555\"}'))], tool_call_id=None, attachments=None), ChatAgentMessage(role='tool', content='{\"format\": \"CSV\", \"value\": \"profile\\\\n\\\\\"all calls are recorded for training and monitoring purposes. I need to file a claim for fire damage in my kitchen from a cooking accident.,   Sorry to hear that, but let\\'s get one thing straight: we don\\'t pay for foolish mistakes. If you\\'re dripped grease on the stove and set the kitchen on fire, that\\'s not our problem. You\\'re gonna need to prove this was some kind of freak accident before we pay up. What\\'s the point of even having insurance if you\\'re just gonna be reckless in the kitchen? Alright, let\\'s get the details. What\\'s the make and model of your vehicle? And what\\'s your policy number?,    I understand your concerns, but I assure you, this was not a foolish mistake. I am a responsible homeowner and I take great care in my cooking. The fire was not caused by dripped grease, but rather a malfunction in my stove\\'s electrical system. I have already had a professional inspect the stove and they have confirmed that it was not my fault.  I would appreciate it if you could take my claim seriously and not assume that I am being reckless. I have been a customer of your company for many years and I expect better service.  Now, can you please tell me what steps I need to take to file my claim? I have all the necessary documentation, including photos of the damage and a detailed list of the items that were damaged in the fire.  And by the way, my policy number is XXXXXXX, and my vehicle\\'s make and model is XXXXXX.  I hope we can resolve this matter in a professional and respectful manner.,   Okay, okay, I get it. You\\'re a responsible homeowner, and this was a legitimate accident. My apologies for assuming the worst. Let\\'s get your claim processed, shall we?  First, I\\'ll need you to provide me with some more information. Can you tell me the date and time of the incident, and a brief description of what happened? And please, spare me the details about the stove\\'s electrical system. I\\'m not an expert, and I don\\'t want to pretend to be one.  Also, I\\'ll need you to submit your documentation, including the photos and list of damaged items. You can upload them to our website or send them to me via email. And please, make sure you include your policy number in the email. We can\\'t process your claim without it.  Once I have all the necessary information, I\\'ll be happy to assist you with your claim. And again, My apologies for my initial response. I\\'m here to help, and I\\'ll do my best to make this process as smooth as possible for you.,  Thank you for your apology and for taking my claim seriously. I appreciate your professionalism and willingness to help.  The date and time of the incident was last Wednesday evening, around  30 pm. I was cooking dinner when I noticed that the stove was emitting a strange, burning smell. I quickly turned off the stove and unplugged it, but it was too late. The kitchen was already filled with smoke, and the flames were starting to spread. I called the fire department immediately, and they arrived within minutes to put out the fire.  As for the documentation, I have taken photos of the damaged area and have a list of the items that were damaged in the fire. I will upload them to your website as soon as possible. My policy number is XXXXXXX.  Thank you again for your help, and I look forward to hearing back from you soon., You\\'re welcome. I\\'m glad to hear that you\\'re taking steps to document the damage and submit your claim. Just to clarify, you mentioned that the stove was emitting a strange, burning smell before the fire started. Have you noticed any other issues with the stove prior to the incident? For example, did you notice any sparks or unusual noises coming from the stove?  Additionally, have you checked to see if there were any recalls on the stove or any similar models? It\\'s possible that there was a manufacturing issue that contributed to the malfunction. We\\'ll definitely want to look into that as part of our investigation.  Lastly, I\\'ll need to know the value of the items that were damaged in the fire. This will help us determine the total amount of damage and process your claim accordingly.  Please keep in mind that this is a lot of information to process, and we\\'ll need some time to investigate and process your claim. But rest assured, we\\'ll do everything we can to make this process as smooth and efficient as possible.\\\\\"\\\\n\", \"truncated\": false}', name='fins_genai__call_center__get_customer_transcript_by_phone_number', id='29c80f65-b3e0-42be-82b8-f325bd6b636f', tool_calls=None, tool_call_id='call_6401c21c-4d98-48a1-adf2-4717f9ed7ba1', attachments=None), ChatAgentMessage(role='assistant', content='', name=None, id='run-731ea4b1-4d37-405f-96ad-bffce36e306a-0', tool_calls=[ToolCall(id='call_69cfb708-e547-44fe-b094-582626fbe031', type='function', function=Function(name='fins_genai__call_center__get_customer_sentiment_by_phone_number', arguments='{\"search_name\": \"(875)-209-8555\"}'))], tool_call_id=None, attachments=None), ChatAgentMessage(role='tool', content='{\"format\": \"CSV\", \"value\": \"profile\\\\nmixed\\\\n\", \"truncated\": false}', name='fins_genai__call_center__get_customer_sentiment_by_phone_number', id='d50f72c3-ab10-47a0-a3c0-3bce102f7ae1', tool_calls=None, tool_call_id='call_69cfb708-e547-44fe-b094-582626fbe031', attachments=None), ChatAgentMessage(role='assistant', content='', name=None, id='run-acc58cdd-371d-4448-b710-aaa5d8c2090e-0', tool_calls=[ToolCall(id='call_f3531725-9ff9-4ea9-8065-72f67afc03c4', type='function', function=Function(name='fins_genai__call_center__get_customer_intent_by_phone_number', arguments='{\"search_name\": \"(875)-209-8555\"}'))], tool_call_id=None, attachments=None), ChatAgentMessage(role='tool', content='{\"error\": \"Failed to execute function with command `spark.sql(SELECT * FROM `fins_genai`.`call_center`.`get_customer_intent_by_phone_number`(:search_name), args={\\'search_name\\': \\'(875)-209-8555\\'})`\\\\nError: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `intent` cannot be resolved. Did you mean one of the following? [`make`, `model`, `sentiment`, `email`, `address`]. SQLSTATE: 42703; line 2 pos 4;\\\\n\\'SQLFunctionNode fins_genai.call_center.get_customer_intent_by_phone_number\\\\n+- \\'SubqueryAlias get_customer_intent_by_phone_number\\\\n   +- \\'Project [cast(getcolumnbyordinal(1, StringType) as string) AS profile#14635]\\\\n      +- \\'LateralJoin lateral-subquery#14637 [], Inner\\\\n         :  +- \\'GlobalLimit 1\\\\n         :     +- \\'LocalLimit 1\\\\n         :        +- \\'Sort [\\'call_timestamp DESC NULLS LAST], true\\\\n         :           +- \\'Project [\\'intent]\\\\n         :              +- Filter (phone_number#14696 = outer(search_name#14636))\\\\n         :                 +- SubqueryAlias fins_genai.call_center.call_center_transcripts_analysis\\\\n         :                    +- Relation fins_genai.call_center.call_center_transcripts_analysis[operator_id#14677,policy_number#14678,policy_type#14679,issue_date#14680,effective_date#14681,expiration_date#14682,make#14683,model#14684,model_year#14685,driver_dob#14686,chassis_no#14687,use_of_vehicle#14688,sum_insured#14689,premium#14690,deductable#14691,address#14692,first_name#14693,last_name#14694,email#14695,phone_number#14696,call_timestamp#14697,call_duration_in_sec#14698,call_time_limit#14699,summary#14700,sentiment#14701,... 14 more fields] parquet\\\\n         +- Project [cast((875)-209-8555 as string) AS search_name#14636]\\\\n            +- OneRowRelation\\\\n\\\\n\\\\nJVM stacktrace:\\\\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\\\\n\\\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:482)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:176)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:426)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:411)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:411)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:411)\\\\n\\\\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\\\\n\\\\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\\\\n\\\\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:411)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\\\\n\\\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\\\n\\\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\\\n\\\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\\\n\\\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\\\n\\\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\\\n\\\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\\\\n\\\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\\\n\\\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\\\n\\\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\\\n\\\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\\\n\\\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\\\n\\\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\\\\n\\\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\\\n\\\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\\\n\\\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\\\n\\\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\\\n\\\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\\\n\\\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\\\\n\\\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\\\n\\\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\\\n\\\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\\\n\\\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\\\n\\\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\\\n\\\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:256)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:414)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkSubqueryExpression(CheckAnalysis.scala:1067)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkSubqueryExpression$(CheckAnalysis.scala:1063)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkSubqueryExpression(Analyzer.scala:414)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:518)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:411)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:411)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:411)\\\\n\\\\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\\\\n\\\\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\\\\n\\\\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:411)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\\\\n\\\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\\\n\\\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\\\n\\\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\\\n\\\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\\\n\\\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\\\n\\\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\\\\n\\\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\\\n\\\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\\\n\\\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\\\n\\\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\\\n\\\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\\\n\\\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\\\\n\\\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\\\n\\\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\\\n\\\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\\\n\\\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\\\n\\\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\\\n\\\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:256)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:414)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:241)\\\\n\\\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\\\n\\\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:228)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:414)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSQLTableFunctions$$anonfun$apply$32.applyOrElse(Analyzer.scala:3773)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSQLTableFunctions$$anonfun$apply$32.applyOrElse(Analyzer.scala:3752)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1320)\\\\n\\\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1319)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:89)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSQLTableFunctions$.apply(Analyzer.scala:3752)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSQLTableFunctions$.apply(Analyzer.scala:3684)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:484)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:633)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:617)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:484)\\\\n\\\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:483)\\\\n\\\\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\\\\n\\\\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\\\\n\\\\tat scala.collection.immutable.List.foldLeft(List.scala:91)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:479)\\\\n\\\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\\\n\\\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:456)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:589)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:589)\\\\n\\\\tat scala.collection.immutable.List.foreach(List.scala:431)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:589)\\\\n\\\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:353)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:506)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:499)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:395)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:499)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:414)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:345)\\\\n\\\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:211)\\\\n\\\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:345)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:486)\\\\n\\\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\\\\n\\\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:486)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:279)\\\\n\\\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\\\n\\\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:525)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:623)\\\\n\\\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:146)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:623)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1257)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:622)\\\\n\\\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:618)\\\\n\\\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1444)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:618)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:273)\\\\n\\\\tat scala.util.Try$.apply(Try.scala:213)\\\\n\\\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1755)\\\\n\\\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1816)\\\\n\\\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:305)\\\\n\\\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:253)\\\\n\\\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:131)\\\\n\\\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1444)\\\\n\\\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1451)\\\\n\\\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\\\n\\\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1451)\\\\n\\\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\\\\n\\\\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1124)\\\\n\\\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1444)\\\\n\\\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1076)\\\\n\\\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3516)\\\\n\\\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3362)\\\\n\\\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3297)\\\\n\\\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\\\\n\\\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\\\\n\\\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\\\\n\\\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:435)\\\\n\\\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1444)\\\\n\\\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:435)\\\\n\\\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\\\\n\\\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\\\\n\\\\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\\\\n\\\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\\\\n\\\\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:434)\\\\n\\\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\\\\n\\\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\\\\n\\\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\\\\n\\\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\\\n\\\\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\\\\n\\\\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\\\\n\\\\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\\\\n\\\\tat scala.util.Using$.resource(Using.scala:269)\\\\n\\\\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\\\\n\\\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\"}', name='fins_genai__call_center__get_customer_intent_by_phone_number', id='4fa4ca77-43ce-4944-b9ea-ac2160ab0d5f', tool_calls=None, tool_call_id='call_f3531725-9ff9-4ea9-8065-72f67afc03c4', attachments=None), ChatAgentMessage(role='assistant', content='', name=None, id='run-406a56ec-304e-4637-b1bb-49e056253ec8-0', tool_calls=[ToolCall(id='call_18d22261-4644-492f-8c26-b873aca8adc1', type='function', function=Function(name='_genai__call_center__get_customer_policy_profile_by_phone_number', arguments='{\"search_name\": \"(875)-209-8555\"}'))], tool_call_id=None, attachments=None), ChatAgentMessage(role='tool', content='{\"format\": \"CSV\", \"value\": \"profile\\\\n\\\\\"The customer Dawn Neal lives in MANHATTAN, 10013.0 with email address ssimon@example.org and phone number(875)-209-8555 owns a AUDI 2019.0 A 3 has a policy of COMP (policy number:102147937) issued on 2023-09-20 and expires on 2024-09-19\\\\\"\\\\n\", \"truncated\": false}', name='_genai__call_center__get_customer_policy_profile_by_phone_number', id='c4caecd5-274e-4465-b537-0df5de4ee5ee', tool_calls=None, tool_call_id='call_18d22261-4644-492f-8c26-b873aca8adc1', attachments=None), ChatAgentMessage(role='assistant', content=\"I have reviewed the customer's profile and the details of their last call. The customer, Dawn Neal, owns a 2019 Audi A3 and has a policy with us (policy number: 102147937) that was issued on September 20, 2023, and expires on September 19, 2024. \\n\\nThe customer's last call was regarding a claim for fire damage in their kitchen due to a cooking accident. The customer's sentiment during the call was mixed. \\n\\nI will make sure to take their claim seriously and assist them with the necessary steps to process it. I will also ensure that our conversation is professional and respectful. \\n\\nPlease let me know if there's anything else I can help you with.\", name=None, id='run-f7e99184-892f-4fa7-829e-70b4d7ef43b5-0', tool_calls=None, tool_call_id=None, attachments=None)], finish_reason=None, custom_outputs=None, usage=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-75132302692d48a3b5487f35826ee7a1\"",
      "text/plain": [
       "Trace(request_id=tr-75132302692d48a3b5487f35826ee7a1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from customer_service_agent import AGENT\n",
    "\n",
    "AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"Write an email response for customer with phone number (875)-209-8555\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7237da3f-c842-48c0-a533-d093af0c12c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Log the agent as code from the `agent.py` file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "### Enable automatic authentication for Databricks resources\n",
    "For the most common Databricks resource types, Databricks supports and recommends declaring resource dependencies for the agent upfront during logging. This enables automatic authentication passthrough when you deploy the agent. With automatic authentication passthrough, Databricks automatically provisions, rotates, and manages short-lived credentials to securely access these resource dependencies from within the agent endpoint.\n",
    "\n",
    "To enable automatic authentication, specify the dependent Databricks resources when calling `mlflow.pyfunc.log_model().`\n",
    "\n",
    "  - **TODO**: If your Unity Catalog tool queries a [vector search index](docs link) or leverages [external functions](docs link), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See docs ([AWS](https://docs.databricks.com/generative-ai/agent-framework/log-agent.html#specify-resources-for-automatic-authentication-passthrough) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/log-agent#resources)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad657dab-6dd9-4dc0-9465-e47aeed1c895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/databricks/connect/session.py:454: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\nWARNING:unitycatalog.ai.core.utils.function_processing_utils:Function name fins_genai__call_center__get_customer_policy_profile_by_phone_number is too long, truncating to 64 characters _genai__call_center__get_customer_policy_profile_by_phone_number.\n2025/04/26 21:37:49 INFO mlflow.pyfunc: Predicting on input example to validate output\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a8b10345-b0de-47ee-b2c5-70ef395d4e3b/lib/python3.10/site-packages/databricks/connect/session.py:454: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\nWARNING:unitycatalog.ai.core.utils.function_processing_utils:Function name fins_genai__call_center__get_customer_policy_profile_by_phone_number is too long, truncating to 64 characters _genai__call_center__get_customer_policy_profile_by_phone_number.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e99a72086f24d9bafb09ba6f9a14713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "from customer_service_agent import tools, LLM_ENDPOINT_NAME\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool in tools:\n",
    "    if isinstance(tool, VectorSearchRetrieverTool):\n",
    "        resources.extend(tool.resources)\n",
    "    elif isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"langgraph_customer_service_agent\",\n",
    "        python_model=\"customer_service_agent.py\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"langchain\",\n",
    "            \"langgraph<0.3.0\",\n",
    "            \"databricks-langchain\",\n",
    "            \"unitycatalog-langchain[databricks]\",\n",
    "            \"pydantic\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c91763d-a392-43e3-b1cb-804f7d17dda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with Agent Evaluation\n",
    "\n",
    "Use Mosaic AI Agent Evaluation to evalaute the agent's responses based on expected responses and other evaluation criteria. Use the evaluation criteria you specify to guide iterations, using MLflow to track the computed quality metrics.\n",
    "See Databricks documentation ([AWS]((https://docs.databricks.com/aws/generative-ai/agent-evaluation) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-evaluation/)).\n",
    "\n",
    "\n",
    "To evaluate your tool calls, add custom metrics. See Databricks documentation ([AWS](https://docs.databricks.com/en/generative-ai/agent-evaluation/custom-metrics.html#evaluating-tool-calls) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/custom-metrics#evaluating-tool-calls))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b01aaf6-93d3-4323-a1f6-0fe1808b2b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_examples = [\n",
    "    {\n",
    "        \"request\": {\"messages\": [{\"role\": \"user\", \"content\": \"Write an email response for customer with phone number (875)-209-8555\"}]},\n",
    "        \"expected_response\": \"\"\"Dear Dawn Neal,\n",
    "I am writing to follow up on our previous conversation regarding the fire damage in your kitchen. I apologize again for any frustration or inconvenience caused by my initial response. I understand that the incident was not a result of reckless behavior, but rather a malfunction in your stove's electrical system.\n",
    "I have reviewed your policy, COMP (policy number: 102147937), and I am happy to inform you that we will proceed with processing your claim. As we discussed earlier, please upload the necessary documentation, including photos of the damaged area and a list of the items that were damaged in the fire, to our website. You can also send them to me via email, making sure to include your policy number.\n",
    "I would like to request additional information to facilitate the investigation. Could you please provide more details about the stove, including its make and model? Have you noticed any other issues with the stove prior to the incident, such as sparks or unusual noises? Additionally, have you checked if there were any recalls on the stove or similar models?\n",
    "To determine the total amount of damage, I will need to know the value of the items that were damaged in the fire. Please provide this information as soon as possible.\n",
    "Please be assured that we will do everything we can to make this process as smooth and efficient as possible. If you have any further questions or concerns, please do not hesitate to reach out to me directly.\n",
    "Thank you for your patience and cooperation.\n",
    "Sincerely, [Your Name]\"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_dataset = pd.DataFrame(eval_examples)\n",
    "display(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340d15cb-ca0f-45bb-984f-6a0e6a1b635f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_id=logged_agent_info.run_id):\n",
    "    eval_results = mlflow.evaluate(\n",
    "        f\"runs:/{logged_agent_info.run_id}/langgraph_customer_service_agent\",\n",
    "        data=eval_dataset,  # Your evaluation dataset\n",
    "        model_type=\"databricks-agent\",  # Enable Mosaic AI Agent Evaluation\n",
    "    )\n",
    "\n",
    "# Review the evaluation results in the MLFLow UI (see console output), or access them in place:\n",
    "display(eval_results.tables['eval_results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee2348c5-c499-4d5b-8990-7899269b08b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pre-deployment agent validation\n",
    "Before registering and deploying the agent, perform pre-deployment checks using the [mlflow.models.predict()](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.predict) API. See Databricks documentation ([AWS](https://docs.databricks.com/en/machine-learning/model-serving/model-serving-debug.html#validate-inputs) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/model-serving-debug#before-model-deployment-validation-checks))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924090ef-d49d-4934-9ad8-348627ebc4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/langgraph_customer_service_agent\",\n",
    "    input_data={\"messages\": [{\"role\": \"user\", \"content\": \"write an email response for customer with phone number (875)-209-8555\"}]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba019ce2-7f3e-461a-b896-372805f19c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Before you deploy the agent, you must register the agent to Unity Catalog.\n",
    "\n",
    "- **TODO** Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ceb4338-7c69-4833-b1be-0daf89366cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"fins_genai\"\n",
    "schema = \"call_center\"\n",
    "model_name = \"langgraph_call_center_cs\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c571a3-dac3-4f21-a5e4-57c891702874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1ea3f0-9281-405f-8d34-9ded7c2bd346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, tags = {\"endpointSource\": \"docs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d0e949-ac33-495a-80a3-7292d4001e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"fins_genai\"\n",
    "schema = \"call_center\"\n",
    "model_name = \"langgraph_call_center_cs\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa1e15c-0005-4b9f-aed0-664bd1462931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001B[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001B[0m\n  color_warning(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d719561f0c47049dd4609fb5717e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9866380ffff247c7bb657bcb9ec5dfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:agents:Failed to create a stable review app URL for serving endpoint 'agents_fins_genai-call_center-langgraph_call_center_cs'. Falling back to the version specific URL.\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/databricks/agents/deployments.py\", line 755, in deploy\n    deployment_info = _create_review_app_and_update_url(deployment_info)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/databricks/agents/deployments.py\", line 947, in _create_review_app_and_update_url\n    experiment_id = client.get_run(model_info.run_id).info.experiment_id\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/mlflow/tracking/client.py\", line 233, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in get_run\n    return self.store.get_run(run_id)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py\", line 177, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py\", line 90, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 392, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 249, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Node ID 2507833646546906 does not exist.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-67ecde30-1a0e-46be-a9fd-0f47d88725fa/lib/python3.10/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001B[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001B[0m\n  color_warning(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Deployment of fins_genai.call_center.langgraph_call_center_cs version 1 initiated.  This can take up to 15 minutes and the Review App & Query Endpoint will not work until this deployment finishes.\n\n    View status: https://adb-984752964297111.11.azuredatabricks.net/ml/endpoints/agents_fins_genai-call_center-langgraph_call_center_cs\n    Review App: https://adb-984752964297111.11.azuredatabricks.net/ml/review/fins_genai.call_center.langgraph_call_center_cs/1?o=984752964297111\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Deployment(model_name='fins_genai.call_center.langgraph_call_center_cs', model_version='1', endpoint_name='agents_fins_genai-call_center-langgraph_call_center_cs', served_entity_name='fins_genai-call_center-langgraph_call_center_cs_1', query_endpoint='https://adb-984752964297111.11.azuredatabricks.net/serving-endpoints/agents_fins_genai-call_center-langgraph_call_center_cs/served-models/fins_genai-call_center-langgraph_call_center_cs_1/invocations', endpoint_url='https://adb-984752964297111.11.azuredatabricks.net/ml/endpoints/agents_fins_genai-call_center-langgraph_call_center_cs', review_app_url='https://adb-984752964297111.11.azuredatabricks.net/ml/review/fins_genai.call_center.langgraph_call_center_cs/1?o=984752964297111')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks import agents\n",
    "agents.deploy(UC_MODEL_NAME, '1', tags = {\"endpointSource\": \"docs\"})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-1-Create-langgraph-tool-calling-agent-customer-service",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
