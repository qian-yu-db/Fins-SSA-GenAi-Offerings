{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0df8479-b76f-4b25-856c-1c587fef1002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# This Notebook Perform the following steps:\n",
    "\n",
    "1. Read a Policy PDF document from a UC volume\n",
    "2. Parsed the PDF documents with PyPDF into text\n",
    "3. Perform Chunking\n",
    "4. Create a vector search index from the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b8f6a1-cd30-4ee5-9740-be03973f15e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet databricks-sdk==0.28.0 databricks-vectorsearch\n",
    "%pip install --quiet pypdf==4.1.0 tiktoken langchain-text-splitters==0.2.2\n",
    "%pip install transformers==4.41.1 torch==2.3.0 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e8e437-ae59-48e0-9d23-1a56d45a697d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c06ab7a-3038-4fbb-a930-5a6294beef67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"table_prefix\", label=\"Source Table\", defaultValue=\"policy_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b53712-a2ab-4633-9d9d-48750d50b8d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_prefix = dbutils.widgets.get(\"table_prefix\")\n",
    "print(f\"table_prefix: {table_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15e0297-d6ef-4083-8b23-fbfaaab88042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {catalog};\")\n",
    "spark.sql(f\"USE SCHEMA {schema};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ae9d2ef-4241-47f5-8181-9591d629e7e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create a Vector Search Index from PDF Files in a Volume Diretory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b672526-1c83-4505-9bc3-2f667e08c86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables_config = {\n",
    "    \"raw_files_table_name\": f\"{table_prefix}_raw_files\",\n",
    "    \"parsed_files_table_name\": f\"{table_prefix}_parsed_files\",\n",
    "    \"chunked_files_table_name\": f\"{table_prefix}_chunked_files\"\n",
    "}\n",
    "\n",
    "embedding_config = {\n",
    "    \"embedding_endpoint_name\": \"databricks-gte-large-en\",\n",
    "    \"embedding_tokenizer\": {\n",
    "            \"tokenizer_model_name\": \"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "            \"tokenizer_source\": \"hugging_face\",\n",
    "        },\n",
    "}\n",
    "\n",
    "chunker_config = {\n",
    "    \"name\": \"langchain_recursive_char\",\n",
    "    \"config\": {\n",
    "        \"chunk_size_tokens\": 1024,\n",
    "        \"chunk_overlap_tokens\": 256\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ede3560a-4085-4be1-8218-1635606cee32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Raw PDF from the UC Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa6e451-058c-40da-8e56-bdd33c7c17a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the raw riles\n",
    "SOURCE_PATH = f\"/Volumes/{catalog}/{schema}/{volume_name_policies}/policy_doc\"\n",
    "\n",
    "raw_files_df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .option(\"pathGlobFilter\", f\"*.pdf\")\n",
    "    .load(SOURCE_PATH)\n",
    ")\n",
    "\n",
    "# Save to a table\n",
    "raw_files_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "    tables_config[\"raw_files_table_name\"]\n",
    ")\n",
    "\n",
    "# reload to get correct lineage in UC\n",
    "raw_files_df = spark.read.table(tables_config[\"raw_files_table_name\"])\n",
    "\n",
    "# For debugging, show the list of files, but hide the binary content\n",
    "display(raw_files_df.drop(\"content\"))\n",
    "\n",
    "# Check that files were present and loaded\n",
    "if raw_files_df.count() == 0:\n",
    "    display(\n",
    "        f\"`{SOURCE_PATH}` does not contain any files.  Open the volume and upload at least file.\"\n",
    "    )\n",
    "    raise Exception(f\"`{SOURCE_PATH}` does not contain any files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c49938-23db-47da-8bec-30db6c9d4fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parse PDF with PyPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6050de2a-bf09-4adf-a86b-4befc5ea94cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from typing import TypedDict, Dict\n",
    "import warnings\n",
    "import io \n",
    "from pyspark.sql.functions import udf, col, md5, explode\n",
    "from pyspark.sql.types import StructType, StringType, StructField, MapType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82a6040-5f32-492d-8fb7-e6fd17cb27b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ParserReturnValue(TypedDict):\n",
    "    doc_parsed_contents: Dict[str, str]\n",
    "    parser_status: str\n",
    "\n",
    "def parse_bytes_pypdf(\n",
    "    raw_doc_contents_bytes: bytes,\n",
    ") -> ParserReturnValue:\n",
    "    try:\n",
    "        pdf = io.BytesIO(raw_doc_contents_bytes)\n",
    "        reader = PdfReader(pdf)\n",
    "\n",
    "        parsed_content = [page_content.extract_text() for page_content in reader.pages]\n",
    "        output = {\n",
    "            \"num_pages\": str(len(parsed_content)),\n",
    "            \"parsed_content\": \"\\n\".join(parsed_content).replace(\"Allstate\", \"Autosure\"),\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"doc_parsed_contents\": output,\n",
    "            \"parser_status\": \"SUCCESS\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Exception {e} has been thrown during parsing\")\n",
    "        return {\n",
    "            \"doc_parsed_contents\": {\"num_pages\": \"\", \"parsed_content\": \"\"},\n",
    "            \"parser_status\": f\"ERROR: {e}\",\n",
    "        }\n",
    "\n",
    "# Create UDF\n",
    "parser_udf = udf(\n",
    "    parse_bytes_pypdf,\n",
    "    returnType=StructType(\n",
    "        [\n",
    "            StructField(\n",
    "                \"doc_parsed_contents\",\n",
    "                MapType(StringType(), StringType()),\n",
    "                nullable=True,\n",
    "            ),\n",
    "            StructField(\"parser_status\", StringType(), nullable=True),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a6fd08-9677-4057-836e-02dc8cbb6898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the parsing\n",
    "parsed_files_staging_df = raw_files_df.withColumn(\"parsing\", parser_udf(\"content\")).drop(\"content\")\n",
    "\n",
    "\n",
    "# Check and warn on any errors\n",
    "errors_df = parsed_files_staging_df.filter(\n",
    "    col(f\"parsing.parser_status\") != \"SUCCESS\"\n",
    ")\n",
    "\n",
    "num_errors = errors_df.count()\n",
    "if num_errors > 0:\n",
    "    print(f\"{num_errors} documents had parse errors.  Please review.\")\n",
    "    display(errors_df)\n",
    "\n",
    "# Filter for successfully parsed files\n",
    "parsed_files_df = parsed_files_staging_df.filter(parsed_files_staging_df.parsing.parser_status == \"SUCCESS\").withColumn(\"doc_parsed_contents\", col(\"parsing.doc_parsed_contents\")).drop(\"parsing\")\n",
    "\n",
    "# Write to Delta Table\n",
    "parsed_files_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tables_config[\"parsed_files_table_name\"])\n",
    "\n",
    "# reload to get correct lineage in UC\n",
    "parsed_files_df = spark.table(tables_config[\"parsed_files_table_name\"])\n",
    "\n",
    "# Display for debugging\n",
    "print(f\"Parsed {parsed_files_df.count()} documents.\")\n",
    "\n",
    "display(parsed_files_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac572c98-68e4-4d35-bb56-5625bce23dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chunk the parsed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f293a15d-11d2-4309-b9cf-b387dcb9638c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db8ed36-6e40-41ed-b815-09deccecdb5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ChunkerReturnValue(TypedDict):\n",
    "    chunked_text: str\n",
    "    chunker_status: str\n",
    "\n",
    "\n",
    "def chunk_parsed_content_langrecchar(\n",
    "    doc_parsed_contents: str, chunk_size: int, chunk_overlap: int, embedding_config\n",
    ") -> ChunkerReturnValue:\n",
    "    try:\n",
    "        tokenizer = tiktoken.encoding_name_for_model('text-embedding-3-large')\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            tokenizer,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.split_text(doc_parsed_contents)\n",
    "        return {\n",
    "            \"chunked_text\": [doc for doc in chunks],\n",
    "            \"chunker_status\": \"SUCCESS\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Exception {e} has been thrown during parsing\")\n",
    "        return {\n",
    "            \"chunked_text\": [],\n",
    "            \"chunker_status\": f\"ERROR: {e}\",\n",
    "        }\n",
    "\n",
    "\n",
    "chunker_udf = udf(\n",
    "    partial(\n",
    "        chunk_parsed_content_langrecchar,\n",
    "        chunk_size=chunker_config.get('config').get(\"chunk_size_tokens\"),\n",
    "        chunk_overlap=chunker_config.get('config').get(\"chunk_overlap_tokens\"),\n",
    "        embedding_config=embedding_config,\n",
    "    ),\n",
    "    returnType=StructType(\n",
    "        [\n",
    "            StructField(\"chunked_text\", ArrayType(StringType()), nullable=True),\n",
    "            StructField(\"chunker_status\", StringType(), nullable=True),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21135fd8-6d4d-41db-b0b7-bcd0618afb35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the chunker\n",
    "chunked_files_df = parsed_files_df.withColumn(\n",
    "    \"chunked\",\n",
    "    chunker_udf(\"doc_parsed_contents.parsed_content\"),\n",
    ")\n",
    "\n",
    "# Check and warn on any errors\n",
    "errors_df = chunked_files_df.filter(chunked_files_df.chunked.chunker_status != \"SUCCESS\")\n",
    "\n",
    "num_errors = errors_df.count()\n",
    "if num_errors > 0:\n",
    "    print(f\"{num_errors} chunks had parse errors.  Please review.\")\n",
    "    display(errors_df)\n",
    "\n",
    "# Filter for successful chunks\n",
    "chunked_files_df = chunked_files_df.filter(chunked_files_df.chunked.chunker_status == \"SUCCESS\").select(\n",
    "    \"path\",\n",
    "    explode(\"chunked.chunked_text\").alias(\"chunked_text\"),\n",
    "    md5(col(\"chunked_text\")).alias(\"chunk_id\")\n",
    ")\n",
    "\n",
    "# Write to Delta Table\n",
    "chunked_files_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "    tables_config[\"chunked_files_table_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9b73233-8887-4960-aca0-c5e19b0543be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Create a Vector Search Index from Chunked Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed019a62-25b0-43cf-96ad-3e55a4cac820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bab8d3d-e839-4fcc-9ed8-44ba23151fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To enable this table as the source of vector search index, we need to enable CDF\n",
    "spark.sql(f\"ALTER TABLE {catalog}.{schema}.{tables_config['chunked_files_table_name']} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01b18ff-f153-42d5-bc6a-3211cbc4c064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{schema}.{tables_config['chunked_files_table_name']}\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{schema}.{tables_config['chunked_files_table_name']}_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    source_table_name=source_table_fullname,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    primary_key=\"chunk_id\",\n",
    "    embedding_source_column='chunked_text', #The column containing our text\n",
    "    embedding_model_endpoint_name='databricks-bge-large-en' #The embedding endpoint used to create the embeddings\n",
    "  )\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6326b647-f0d5-41d3-9975-e42a91c01bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-Create-vs-Index-Policy-Doc",
   "widgets": {
    "table_prefix": {
     "currentValue": "policy_docs",
     "nuid": "1283344a-a26a-41db-bd6c-4b9559a0676f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "policy_docs",
      "label": "Source Table",
      "name": "table_prefix",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "policy_docs",
      "label": "Source Table",
      "name": "table_prefix",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
